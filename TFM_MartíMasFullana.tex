\documentclass[a4paper,12pt,twoside]{ThesisStyle}
\usepackage[utf8]{inputenc}
\usepackage{thesis-style}
\usepackage[english]{babel}

\begin{document}

\frontmatter

\pagenumbering{gobble}

\thispagestyle{empty}
\begin{table}[htb]
  \centering
  \begin{Large}
    \resizebox{\textwidth}{!}{\begin{tabular}{ | l |}
        \hline
        \\
        \includegraphics[scale=0.9]{imatges/logo_eps.png}    \\[0.7cm]
        \centerline{Treball Final de Màster}                 \\[1cm]
        \hline
        \\
        Estudi: Màster en Ciència de Dades                   \\[0.7cm]
        \hline
        \\
        Títol: Ajustament d'un model generatiu de llenguatge \\
        per a la creació de xatbots personalitzats per       \\
        administracions públiques                            \\[0.7cm]
        \hline
        \\
        Document: Memòria                                    \\[0.7cm]
        \hline
        \\
        Alumne: Martí Mas Fullana                            \\[0.7cm]
        \hline
        \\
        Tutor: Josep Suy Franch                              \\
        Tutor: Miquel Tarragona Margarit                     \\[0.7cm]
        Departament: Departament d'Informàtica, Matemàtica   \\
        Aplicada i Estadística                               \\
        Àrea: Intel·ligència Artificial                      \\[0.7cm]
        \hline
        \\
        Convocatòria (mes/any): Setembre 2024                \\[0.7cm]
        \hline
      \end{tabular}}
  \end{Large}
\end{table}

\newpage
\hypersetup{pageanchor=false}
\begin{titlepage}

  % Upper part of the page
  \includegraphics[scale=0.9]{imatges/logo_eps.png} \\[1cm]
  \begin{center}
    \textsc{\Large Master's Thesis} \\[1cm]

    % Title
    \begin{spacing}{2}
      \HRule \\
      \textbf{\Huge Tuning a Generative Language Model for the Creation of Customized Chatbots for Public Administrations} \\
      \HRule \\[0.5cm]
    \end{spacing}

    % Author and supervisor and other data
    {
    \large
    \emph{Author:} \\
    Martí \textsc{Mas Fullana} \\[1cm]
    Setembre 2024 \\[1cm]
    Master's in Data Science \\[1cm]
    \emph{Supervisor:} \\
    Josep \textsc{Suy Franch} \\
    }

  \end{center}
\end{titlepage}
\hypersetup{pageanchor=true}

\titlepage

%\dominitoc


\pagenumbering{roman}

\chapter*{Summary}
\label{cap:summary}

% This document presents the architecture and methodology for developing an advanced chatbot system that uses GPT (Generative Pre-trained Transformer) technology and RAG (Retrieval Augmented Generation) to provide assistance with social rights and benefits in Catalonia. The system aims to improve the accuracy and relevance of responses generated by chatbots by combining the strengths of information retrieval from a database with the generative capacity of language models. The system consists of a frontend interface implemented in Angular, a backend that manages the conversation flow and communicates with Azure services such as Speech-to-Text and a PostgreSQL database, and a Vector Search API that handles information retrieval and response generation. The methodology involves data collection and preparation, implementation of the RAG system, user interface design, and evaluation and validation of the system. The objectives of the project include selecting an appropriate GPT model, integrating RAG technology, facilitating user-chatbot interaction, ensuring accessibility, and evaluating and validating the system. The document also provides background information on dialogue systems, GPT, and RAG, as well as the objectives and architecture of the project.

This document presents the design and implementation of a customized chatbot system utilizing GPT (Generative Pre-trained Transformer) and RAG (Retrieval Augmented Generation) technologies to enhance public administration services in Catalonia, specifically for social rights and benefits. The system integrates a frontend interface in Angular with a backend that manages conversational flow and connects with Azure services such as Speech-to-Text and a PostgreSQL database. The backend also employs a Vector Search API for information retrieval and response generation. Key components include data collection and preparation from public websites, implementation of the RAG framework for precise information retrieval, and the development of a user-friendly, multilingual interface with accessibility features. The system's architecture is modular, enabling adaptability for future applications, and is validated through user testing and performance analysis, aiming to improve accuracy, relevance, and user satisfaction while minimizing operational costs.

\chapter*{Acknowledgements}
\label{cap:acknowledgements}

I would like to express my gratitude to my tutor Josep Suy Franch for his guidance and support throughout the development of this project. I would also like to thank Benjami Fuertes, Sergi Martinez and Joan Oller for their hard work and dedication to the project, and for their patience answering my questions, as well as thank DXC Technology for providing me with this opportunity. I thank my family and friends for their encouragement and understanding during this time.
Finally, I would like to thank my fiancée for her unwavering love and support.


\tableofcontents

\listoffigures

\listoftables

\mainmatter

\chapter{Introduction}
\label{cap:intro}

We present a chatbot system that uses GPT (Generative Pre-trained Transformer) technology and RAG (Retrieval Augmented Generation) to provide assistance with social rights and benefits in Catalonia. Our client is the department of social rights of the Generalitat de Catalunya (\textit{Departament de Drets Socials} or DSO).

\section{Key Terms and Definitions}
\label{sec:terms}

\begin{itemize}
  \item \textbf{DSO:} \textit{Departament de Drets Socials}, tghe Department of Social Rights of the Generalitat de Catalunya. This department is responsible for managing social benefits and services in Catalonia.
  \item \textbf{SNOMED:} A SNOMED, short for Systematized Nomenclature of Medicine, is a code that represents a medical concept. For example, the SNOMED code for ``diabetes'' is 73211009.
  
  For reasons not relevant to this project, the Department of Social Rights uses SNOMEDs to represent a user's situation. These SNOMEDs are used to determine which social benefits a user might be eligible for. For example, they have a SNOMED for ``Sexist Violence at Home''. As we will see in chapter \ref{cap:architecture}, we have a component called \textit{IA Social} that can infer which SNOMEDs might apply to a user based on the conversation.
  \item \textbf{Products:} These are the actual social benefits that the user might be eligible for. These might entail things like monetary aid, legal assistance, or other forms of support. The products are determined by the SNOMEDs that apply to the user. The Department of Social Rights provides us with an API called ``kSocial'' (``k'' is for ``Katalog'') that we can use to query which products are available for a given set of SNOMEDs.
\end{itemize}

\chapter{Background}
\label{cap:background}

\section{Introduction to Dialogue Systems}
\label{sec:chat}

Dialogue systems, also known as chatbots, have experienced a significant step-change in the last few years. Initially these systems were based on predefined rules and decision trees \cite{Weizenbaum1966ELIZA, AbuShawar2015ALICE}, limiting their capacity for understanding and answering user queries in a natural and flexible manner. These rudimentary systems, commonly referenced as rule-based chatbots, might have been enough for simple tasks, but could not have managed the full complexity and variability of natural language.

\section{Towards Language Models}
\label{sec:language}

As the first machine learning-based language models appeared, such as the Sequence-to-Sequence (Seq2Seq) model \cite{Sutskever2014SequenceSequenceLearningNeural}, and more recently the transformer-based models such as GPT (Generative Pre-trained Transformer) \cite{Vaswani2023AttentionNeed, Radford2018ImprovingLU}, the capacity of chatbots to understand and generate natural language has improved significantly. These models are trained on large datasets of text, learning the complex patterns and structures of language, and are able to generate text that is coherent and contextually relevant.

\section{GPT and its Contribution}
\label{sec:gpt}

The GPT model \cite{Radford2018ImprovingLU}, developed by OpenAI, has been one of the most notable advances in this field. GPT uses the transformer architecture \cite{Vaswani2023AttentionNeed}, which is a type of neural network that is particularly well-suited for processing sequences of data, such as text. Its capacity for generating coherent and contextually relevant responses has been leveraged in a wide range of applications, from virtual assistance to automated content generation.

\section{Retrieval Augmented Generation (RAG)}
\label{sec:rag}

One of the most recent advances in the integration of language models has been the use of retrieval augmented generation (RAG) \cite{Lewis2021RetrievalAugmentedGeneration}. RAG combines the strengths of information retrieval from databases with the generative capacity of language models. In this context, when a user query is received, the system first retrieves relevant information from a database, and then the language model generates a coherent and precise response based on this information. This approach has been shown to improve the accuracy and relevance of the responses generated by chatbots \cite{Lewis2021RetrievalAugmentedGeneration}.

\section{Applications and Benefits of RAG-based Chatbots}
\label{sec:applications}

RAG-based chatbots offer a variety of benefits compared with more traditional systems. They are able to generate responses that are more coherent and contextually relevant. In this way users are both less frustrated and more satisfied. These systems also allow the chatbots to have access to newer, more up to date information than the data the model was originally trained on, as the data provided to the information retrieval component can be updated by simply adding new entries to the database. This makes the chatbot more adaptable and flexible, and allows it to provide more accurate and relevant information to users, reducing the necessity of performing full or partial retraining of the model, which can be prohibitively expensive.

\chapter{Objectives}
\label{cap:objectives}

The main goal of this project is to develop an advanced chatbot system that uses GPT (Generative Pre-trained Transformer) technology and RAG (Retrieval Augmented Generation) to provide responses to user queries based off of the content of a database. This general goal can be broken down into the following specific objectives:

\begin{enumerate}
  \item \textbf{Pick an appropriate GPT Model}
        \begin{itemize}
          \item Choose a GPT model that is well-suited for the task of generating responses to user queries based on the content of a database.
        \end{itemize}
  \item \textbf{Integrate RAG Technology}
        \begin{itemize}
          \item \textbf{Information Retrieval:} Develop and implement a system for retrieving relevant information from a database based on user queries.
          \item \textbf{Combine Retrieval and Generation:} Integrate the information retrieval system with the GPT model to generate coherent and contextually relevant responses to user queries.
        \end{itemize}
  \item \textbf{Facilitate User-Chatbot Interaction}
        \begin{itemize}
          \item \textbf{UI Design} Develop a user interface that allows users to interact with the chatbot in a natural and intuitive way.
          \item \textbf{UX Design} Ensure that the user experience is smooth and seamless, and that users are able to easily access the information they need.
        \end{itemize}
  \item \textbf{Accessibility}
        \begin{itemize}
          \item \textbf{Multilingual Support} Implement support for multiple languages to make the chatbot accessible to a wider range of users.
          \item \textbf{Accessibility Features} The system must be designed to be accessible to users with visual or motor impairments. As such, it should support voice input. The voice input feature must be able to be activated through a voice command.
        \end{itemize}
  \item \textbf{Evaluate and Validate the System}
        \begin{itemize}
          \item \textbf{User Testing} Conduct user testing to assess the usability and effectiveness of the chatbot system.
          \item \textbf{Results Analysis} Analyze the results of the different tests to identify areas for improvement and optimization.
        \end{itemize}
\end{enumerate}

\chapter{Methodology}
\label{cap:methodology}

\section{Data Collection and Preparation}
\label{sec:data}

\begin{itemize}
  \item \textbf{Data Collection:} We are given by the stakeholders a set of websites documenting the laws related to social rights in Catalonia and also documenting available social benefits. These websites contain a variety of information, including the text of the laws and the social benefits, what conditions are necessary to access them, and how to apply for them.
  \item \textbf{Data Preparation:} We will extract the text from the websites using web scraping and convert them into a format that can be used by the information retrieval system. This will involve cleaning the text and removing any extraneous characters. This will be done using custom web scraping scripts.
\end{itemize}

\section{RAG Implementation}
\label{sec:rag-implementation}

\begin{itemize}
\item \textbf{Information Retrieval:} Develop a system capable of retrieving relevant information from a database based on user queries. This will involve creating an index of the laws related to social rights and available social benefits in Catalonia, and implementing a search algorithm that can return the most relevant laws based on the user query. In practice this will be done using the LlamaIndex Python library, which chunks the text into smaller pieces and indexes them.
  \item \textbf{Combining Retrieval and Generation:} Integrate the retrieval system with the GPT model to generate coherent and contextually relevant responses to user queries. This will involve passing the retrieved information to the GPT model, which will generate a response based on this information.
\end{itemize}

\section{User Interface Design}
\label{sec:ui-design}

\begin{itemize}
  \item \textbf{UI Design:} Develop a user interface that allows users to interact with the chatbot in a natural and intuitive way. This will involve creating a chat interface that allows users to input queries and receive responses from the chatbot.
  \item \textbf{UX Design:} Ensure that the user experience is smooth and seamless, and that users are able to easily access the information they need. This will involve testing the user interface with a group of users to identify any areas for improvement, as well as demoing the system to the stakeholders.
\end{itemize}


\chapter{Architecture}
\label{cap:architecture}

We develop an Angular Frontend conversation UI (similar to other messaging apps) that communicates with a Backend that manages the calls to the language models and the database. The Backend also handles the management of the database and the indexing of the information. The diagram of the app's architecture is shown in Figure \ref{fig:architecture}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\textwidth]{imatges/Full DSO App Architecture.drawio.png}
  \caption{App Architecture}
  \label{fig:architecture}
\end{figure}

Over the following sections we will describe the different components of the system in more detail, and how a user's interaction is processed through the system.

\section{Frontend}
\label{sec:frontend}

The frontend is implemented in Angular, a popular web development framework. The frontend is responsible for managing the user interface and handling the user's interactions with the chatbot. The frontend communicates with the backend to send user queries and receive responses from the chatbot. It also communicates with the Azure Speech-to-Text service to convert audio data into text. Figure \ref{fig:frontend} shows an example of the frontend interface.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\textwidth]{imatges/UI Design.png}
  \caption{Frontend Interface}
  \label{fig:frontend}
\end{figure}


\section{Hotword/Wakeword Detection}
\label{sec:hotword_detection}

On the frontend we implement a custom version of the EfficientWord-Net \cite{Chidhambararajan2022EfficientWordNet} hotword detector, that has been quantized \cite{Zhang2023PostTrainingQuantizationNeuralNetworks}. To implement it we use ONNX \cite{onnx} and WebAssembly (Wasm) \cite{wasm}, which run directly in the browser, ensuring no data leaves the client's machine inadvertently. The hotword detector listens to the microphone data in real-time, and when it hears the keyword ``chat'' or ``chatbot'', it activates the microphone and starts recording. We will see how this data is processed in the \ref{sec:azure_services} section.

The EfficientWord-Net model has been quantized to 8-bits, which has reduced the size of the model from 80MB to 20MB, and has improved the response time by 100\%. This has been achieved without perceptibly sacrificing the accuracy of the hotword detection.

The hotword detector analyzes audio data in chunks of 1.5 seconds, overlapped by 0.75 seconds. The raw audio signal is first converted to a Mel spectrogram (Figure~\ref{fig:mel_spectrogram}), which is then passed through a ResNet \cite{He2015DeepResidualLearningImage} model to generate semantic embedding vectors. These vectors are then compared to the embedding vectors of reference recordings of the hotword (which are prerecorded) using cosine similarity. If the similarity is above a certain threshold, the hotword is detected.
We use the pretrained model as provided by the EfficientWord-Net library, with no further training.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\textwidth]{imatges/MelSpectrogram.png}
  \caption{Mel spectrogram of the audio ``Hello world'' (image taken from \cite{Chidhambararajan2022EfficientWordNet})}
  \label{fig:mel_spectrogram}
\end{figure}

Because this system needs to run on the browser, and there is no existing implementation of a Mel spectrogram converter, we implement our own converter directly in TypeScript. This converter is as close as possible to a direct translation --from Python to TypeScript-- of the original code from the EfficientWord-Net library. By doing so we ensure data generated by our implementation is equivalent to that generated by the original implementation. We evaluate that this is the case by making bitwise comparisons of the output of both implementations (subtracting one image from the other), and find there are no differences (all pixels in the resulting image are exactly 0).

On our reference system, which consists of a Dell Latitude 3440 laptop with a 13th Gen Intel(R) Core(TM) i5-1345U CPU, the hotword detector has a response time of around 80-100 milliseconds, which is well within the acceptable range for real-time applications.

\section{Azure Services}
\label{sec:azure_services}

\subsubsection{Speech-to-Text}

When the hotword detector activates the microphone, the audio data is sent to the Azure Speech-to-Text service. This service converts the audio data into text, which is then sent to the Backend for processing. The Azure Speech-to-Text service uses the Whisper \cite{Radford2022RobustSpeechRecognitionLargeScale} model to accurately transcribe speech into text.

\subsection{PostgreSQL Database}
\label{subsec:database}

The PostgreSQL database contains the text of the laws related to social rights and available social benefits in Catalonia. This data is indexed using the LlamaIndex Python library to chunk the text into smaller pieces, index them and  help create semantic embeddings. When a user query is received, the information retrieval system searches the database for the most relevant laws and benefits based on the query, and returns this information to the GPT model for generation.

\subsection{GPT Model}
\label{subsec:gpt_model}

The GPT model is used to generate coherent and contextually relevant responses to user queries based on the information retrieved from the database. The GPT model is a transformer-based \cite{Vaswani2023AttentionNeed} language model that is trained on a large dataset of text to generate human-like responses to user queries.

We use the GPT-4o and GPT-4o Mini models, which are the latest versions of the GPT model developed by OpenAI at the time of writing.

\section{Backend}
\label{sec:backend}

\subsection{Conversation Flow}
\label{subsec:conversation_flow}

The Backend implements, the conversation flow followed by the chatbot. It implements multiple stages and follows a state machine to manage the conversation. User queries at each stage are routed to the appropriate model.

Conversations flows are implemented using Flowable. Figures \ref{fig:conversation}, \ref{fig:antidan}, \ref{fig:treatscenario}, \ref{fig:answerragquestionifexists}, \ref{fig:executesituationdiscoveryscenario}, and \ref{fig:infervariablesprocess} show the flow diagrams for the different stages of the conversation.

At each stage the backend can decide to talk to one of the other components of the architecture. For example, during the RAG stage it uses our Vector Search API component to generate a response based on the information retrieved from the database. In another case, in the scenario where we are discovering the user's situation, the backend talks to the IA Social component to and asks it if, given the current conversation, there are any SNOMEDs that might apply to the user.

The backend also implements any error handling that might be necessary. The error handling is implemented directly on the flow diagram itself.

\section{Vector Search API}
\label{sec:vector_search_api}

As we needed to implement a few custom components all related to the RAG system, we decided to abstract them into a single separate component, the Vector Search API. This component is responsible for managing the information retrieval system, and for generating responses based on the information retrieved from the database. This component is abstract enough to be reused in chatbot systems other than the one we are developing for the DSO.

This API is implemented in Python using Flask to serve the API endpoints, and with Waitress as the WSGI server, as depicted in Figure \ref{fig:architecture}.

The following sections describe the features that this API provides.

\subsection{Model Routing}
\label{subsec:model_routing}

We use a slightly tweaked version of the fairly new RouteLLM Python library. This allows us to route a certain percentage of queries to a ``stronger'', more expensive model and the rest to a ``weaker'', less expensive model. With this system we are able to achieve X\% of the performance of the stronger model at a fraction of the cost. The RouteLLM system uses a small BERT classifier model that decides which queries should be routed to the stronger model and which to the weaker model. This classifier was trained by the original library authors on a dataset of human preferences augmented with synthetic data generated using GPT-4. They report good generalization performance, so we apply the system on a pair consisting of GPT-4o and GPT-4o Mini. We have also made the necessary changes to the library to make it compatible with the Azure OpenAI models, which didn't have official support.

We have translated the dataset the authors used to train the BERT classifier to Catalan, and are in the process of retraining the classifier on this new dataset, at the time of writing.

The translated training dataset consists of two parts: the translated texts and the embeddings of the texts. The translations were generated using the GPT-4o model and the embeddings using the \textit{text-embedding-3-large} model. The total cost of generating the translations and embeddings was around 500 euros. The datasets can be found here \href{https://huggingface.co/datasets/SupremeLobster/gpt4_judge_battles_catalan}{[translated texts]} and here \href{https://huggingface.co/datasets/SupremeLobster/gpt4_judge_battles_catalan_embeddings}{[embeddings]}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{imatges/Routed conversation.png}
  \caption{Example of conversation messages being routed to different models according to their complexity}
  \label{fig:model_routing_example}
\end{figure}

\subsection{Information Retrieval}
\label{subsec:information_retrieval}

We do RAG using the LlamaIndex python library.

We use two versions of RAG: normal RAG and Small To Big Retrieval (STBR).

\begin{itemize}
  \item \textbf{Normal RAG:} This is the standard RAG algorithm. It simply creates an embedding of the user query and does cosine similarity with the embeddings of the chunks in the database to retrieve the N most relevant chunks. In practice the database implements this with the Hierarchical Navigable Small World (HNSW) \cite{Malkov2018EfficientRobustApproximateNearestNeighbors} algorithm, which is a fast approximate nearest neighbor search algorithm, in order to avoid having to compare the user query with all the entries in the database's table. However this implementation is invisible, as the database itself executes it.
  \item \textbf{Small To Big Retrieval (STBR):} This is a less common RAG algorithm that uses hierarchical embeddings of chunks, allowing us to capture both fine-grained and coarse-grained information. Figure \ref{fig:SmallToBigRetrieval} shows our hierarchy-of-embeddings setup. Each level's embeddings correspond to smaller and smaller chunks of the text. We have 3 levels of embeddings. This can be thought of as analogous to the way a CNN model captures both fine-grained and coarse-grained information, where deeper layers have a larger receptive field. We implement the algorithm using the \href{https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes/}{RecursiveRetriever class} from the LlamaIndex library.
  
  The STBR algorithm is implemented in the Vector Search API and runs at the top level, so the embedding searches are still performed by the database itself, still using the HNSW algorithm. Therefore STBR is more expensive because it performs multiple database queries (one per hierarchy level), but it is also more accurate for retrieving the most relevant information.
\end{itemize}

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\textwidth]{imatges/Small To Big Retrieval.drawio.png}
  \caption{Small To Big Retrieval (STBR) embeddings hierarchy}
  \label{fig:SmallToBigRetrieval}
\end{figure}

\chapter{Results}
\label{cap:results}

RESULTSSSS

\backmatter

% \bibliographystyle{ThesisStyleBreakable}
\bibliographystyle{ieeetr}
\bibliography{biblio}

%\printnomenclature


\appendix
\include{Appendix1}

\chapter{Flowable Diagrams}
\label{cap:flowable_diagrams}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{imatges/Conversation_process.bpmn20.png}
  \caption{Top level flow diagram for the conversation}
  \label{fig:conversation}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{imatges/Execute_anti_DAN.bpmn20.png}
  \caption{Flow diagram for the Anti DAN stage}
  \label{fig:antidan}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{imatges/Treat_conversation_scenario.bpmn20.png}
  \caption{Flow diagram for treating a scenario}
  \label{fig:treatscenario}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{imatges/Answer_question_if_exists.bpmn20.png}
  \caption{Flow diagram for the answering a question using RAG (if a question exists)}
  \label{fig:answerragquestionifexists}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{imatges/Situation_discovery_scenario.bpmn20.png}
  \caption{Flow diagram for the scenario where we need to discover the user's situation}
  \label{fig:executesituationdiscoveryscenario}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{imatges/Infere_conversation_variables.bpmn20.png}
  \caption{Flow diagram for inferring necessary variables for the current scenario based on the conversation}
  \label{fig:infervariablesprocess}
\end{figure}

\chapter{Our Contributions to Open-Source}
\label{cap:oss_contributions}

During the development of this project we have iterated and made changes to a few existing tools and components. Here we link to our forks of the repositories.

\begin{itemize}
  \item EfficientWord-Net: \url{https://github.com/SupremeLobster/EfficientWord-Net} - Our fork of the EfficientWord-Net library, which has been modified to reduce response time, client machine requirements, and the size of downloaded files, through Quantization methods.
  \item RouteLLM: \url{https://github.com/SupremeLobster/RouteLLM} - Our fork of the RouteLLM library, which allows us to route a certain percentage of queries to a ``stronger'', more expensive model and the rest to a ``weaker'', less expensive model. Our changes are in the branch ``feature/support-azure-openai-embeddings''. These changes made the library compatible with the Azure OpenAI models, which didn't have official support.
\end{itemize}

\end{document}
